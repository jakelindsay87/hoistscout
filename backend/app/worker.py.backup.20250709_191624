from celery import Celery
from celery.schedules import crontab
import asyncio
from typing import Dict, Any
from datetime import datetime
import logging
import traceback

from .config import get_settings

# Configure logging with more detail
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

settings = get_settings()

# Log configuration
logger.info("=== CELERY WORKER INITIALIZATION ===")
logger.info(f"Worker starting with Redis URL: {settings.redis_url}")
logger.info(f"USE_GEMINI: {settings.use_gemini}")
logger.info(f"GEMINI_API_KEY configured: {'Yes' if settings.gemini_api_key else 'No'}")

# Create Celery app
celery_app = Celery(
    "hoistscout",
    broker=settings.redis_url,
    backend=settings.redis_url
)

# Import tasks after app creation to avoid circular import
celery_app.autodiscover_tasks(['app'], related_name='worker')

# Make celery_app available as 'celery' for the celery command
celery = celery_app
# Also keep worker alias for backward compatibility
worker = celery_app

# Log Celery app creation
logger.info(f"Celery app created with broker: {celery_app.conf.broker_url}")
logger.info(f"Celery app name: {celery_app.main}")

# Configure Celery
celery_app.conf.update(
    task_serializer="json",
    accept_content=["json"],
    result_serializer="json",
    timezone="UTC",
    enable_utc=True,
    task_track_started=True,
    task_time_limit=30 * 60,  # 30 minutes
    task_soft_time_limit=25 * 60,  # 25 minutes
    worker_prefetch_multiplier=1,
    worker_max_tasks_per_child=1000,
    task_default_queue="celery",
    task_create_missing_queues=True,
)

# Log final configuration
logger.info("=== CELERY CONFIGURATION ===")
logger.info(f"Default queue: {celery_app.conf.task_default_queue}")
logger.info(f"Create missing queues: {celery_app.conf.task_create_missing_queues}")
logger.info(f"Task serializer: {celery_app.conf.task_serializer}")
logger.info(f"Task time limit: {celery_app.conf.task_time_limit}s")

# Configure periodic tasks
celery_app.conf.beat_schedule = {
    "scrape-all-active-websites": {
        "task": "app.worker.scrape_all_active_websites",
        "schedule": crontab(hour="*/6"),  # Every 6 hours
    },
    "cleanup-old-jobs": {
        "task": "app.worker.cleanup_old_jobs",
        "schedule": crontab(hour=2, minute=0),  # Daily at 2 AM
    },
}

# Log registered tasks
logger.info("=== REGISTERED CELERY TASKS ===")
for task_name in celery_app.tasks:
    logger.info(f"- {task_name}")


@celery_app.task(bind=True, name='app.worker.scrape_website_task', max_retries=3)
def scrape_website_task(self, website_id: int):
    """Scrape a single website."""
    logger.info(f"=== SCRAPE_WEBSITE_TASK STARTED ===")
    logger.info(f"Task ID: {self.request.id}")
    logger.info(f"Website ID: {website_id}")
    logger.info(f"Retry count: {self.request.retries}")
    logger.info(f"Queue: {self.request.delivery_info.get('routing_key', 'unknown')}")
    
    try:
        # Run async scraping in sync context
        logger.info("Creating new event loop for async operations...")
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        from .database import AsyncSessionLocal
        from .models.website import Website
        from .models.job import ScrapingJob, JobStatus
        from .models.opportunity import Opportunity
        from sqlalchemy import select
        
        async def run_scraping():
            async with AsyncSessionLocal() as db:
                # Get website
                stmt = select(Website).where(Website.id == website_id)
                result = await db.execute(stmt)
                website = result.scalar_one_or_none()
                
                if not website:
                    raise ValueError(f"Website {website_id} not found")
                
                # Update job status
                job_id = self.request.id
                job = None
                if job_id:
                    # Try to convert job_id to integer if it's numeric
                    try:
                        # Check if the job_id is a numeric string
                        if job_id.isdigit():
                            numeric_job_id = int(job_id)
                            job_stmt = select(ScrapingJob).where(ScrapingJob.id == numeric_job_id)
                            job_result = await db.execute(job_stmt)
                            job = job_result.scalar_one_or_none()
                            if job:
                                job.status = JobStatus.RUNNING
                                job.started_at = datetime.utcnow()
                                await db.commit()
                        else:
                            # Job ID is not numeric, likely a Celery UUID
                            logger.warning(f"Job ID '{job_id}' is not numeric, skipping job status update")
                    except (ValueError, AttributeError) as e:
                        logger.warning(f"Could not convert job ID '{job_id}' to integer: {e}")
                
                # Use Gemini scraper directly for production
                try:
                    from .core.gemini_scraper import GeminiScraper
                    
                    logger.info(f"Starting Gemini scraper for website {website_id}")
                    
                    # Create Gemini scraper
                    scraper = GeminiScraper()
                    
                    # Execute scraping
                    result = await scraper.scrape_website(website)
                    
                    logger.info(f"Scraping completed: {result.total_found} opportunities found")
                    
                except Exception as e:
                    logger.error(f"Gemini scraping failed: {e}")
                    # Return error result
                    from datetime import datetime
                    result = type('obj', (object,), {
                        'opportunities': [],
                        'success': False,
                        'error_message': f"Scraping error: {str(e)}",
                        'stats': {
                            'error': str(e),
                            'timestamp': datetime.utcnow().isoformat()
                        }
                    })
                
                # Save opportunities
                for opp_data in result.opportunities:
                    opportunity = Opportunity(
                        website_id=website_id,
                        title=opp_data.get("title"),
                        description=opp_data.get("description"),
                        deadline=opp_data.get("deadline"),
                        value=opp_data.get("value"),
                        currency=opp_data.get("currency", "USD"),
                        reference_number=opp_data.get("reference_number"),
                        source_url=opp_data.get("source_url"),
                        categories=opp_data.get("categories", []),
                        location=opp_data.get("location"),
                        extracted_data=opp_data,
                        confidence_score=opp_data.get("confidence_score", 1.0)
                    )
                    db.add(opportunity)
                
                # Update job status
                if job:
                    job.status = JobStatus.COMPLETED
                    job.completed_at = datetime.utcnow()
                    job.stats = result.stats
                
                await db.commit()
                
                logger.info(f"Database commit successful for website {website_id}")
                return result.stats
        
        logger.info("Starting async scraping operation...")
        result = loop.run_until_complete(run_scraping())
        logger.info(f"=== SCRAPE_WEBSITE_TASK COMPLETED SUCCESSFULLY ===")
        logger.info(f"Result: {result}")
        return result
        
    except Exception as exc:
        logger.error(f"=== SCRAPE_WEBSITE_TASK FAILED ===")
        logger.error(f"Error type: {type(exc).__name__}")
        logger.error(f"Error message: {str(exc)}")
        logger.error(f"Traceback: {traceback.format_exc()}")
        
        # Log error and retry
        retry_countdown = 60 * (self.request.retries + 1)
        logger.info(f"Retrying task in {retry_countdown} seconds...")
        self.retry(exc=exc, countdown=retry_countdown)


@celery_app.task
def process_pdf_task(document_id: int):
    """Process a PDF document."""
    try:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        from .database import AsyncSessionLocal
        from .models.opportunity import Document
        from sqlalchemy import select
        
        async def run_processing():
            async with AsyncSessionLocal() as db:
                # Get document
                stmt = select(Document).where(Document.id == document_id)
                result = await db.execute(stmt)
                document = result.scalar_one_or_none()
                
                if not document:
                    raise ValueError(f"Document {document_id} not found")
                
                # Process PDF (lazy import)
                try:
                    from .core.pdf_processor import PDFProcessor
                    processor = PDFProcessor()
                    # Implement PDF processing logic
                except (ImportError, ModuleNotFoundError) as e:
                    # PDF processing not available
                    pass
                
                await db.commit()
        
        loop.run_until_complete(run_processing())
        
    except Exception as exc:
        raise


@celery_app.task
def scrape_all_active_websites():
    """Periodic task to scrape all active websites."""
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    
    from .database import AsyncSessionLocal
    from .models.website import Website
    from sqlalchemy import select
    
    async def run_task():
        async with AsyncSessionLocal() as db:
            stmt = select(Website).where(Website.is_active == True)
            result = await db.execute(stmt)
            websites = result.scalars().all()
            
            for website in websites:
                scrape_website_task.delay(website.id)
    
    loop.run_until_complete(run_task())


@celery_app.task
def cleanup_old_jobs():
    """Clean up old completed jobs."""
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    
    from .database import AsyncSessionLocal
    from .models.job import ScrapingJob, JobStatus
    from sqlalchemy import select, and_
    from datetime import datetime, timedelta
    
    async def run_cleanup():
        async with AsyncSessionLocal() as db:
            cutoff_date = datetime.utcnow() - timedelta(days=30)
            
            stmt = select(ScrapingJob).where(
                and_(
                    ScrapingJob.status.in_([JobStatus.COMPLETED, JobStatus.FAILED]),
                    ScrapingJob.created_at < cutoff_date
                )
            )
            result = await db.execute(stmt)
            old_jobs = result.scalars().all()
            
            for job in old_jobs:
                await db.delete(job)
            
            await db.commit()
    
    loop.run_until_complete(run_cleanup())